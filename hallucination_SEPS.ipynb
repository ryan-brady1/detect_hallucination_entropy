{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKq2T4r4OAxb"
      },
      "source": [
        "<h1>Semantic Entropy Probes: Robust and Cheap\n",
        "Hallucination Detection in LLMs</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hmbv8PHULEq"
      },
      "source": [
        "<p style=\"font-zize: 16px\"> <a href=\"https://arxiv.org/abs/2406.15927\">Pre Published paper.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srt_fnitH8yM"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2TWQU5K48j"
      },
      "outputs": [],
      "source": [
        "!pip install openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqKyZCFeHs_u"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFHb908ZUzLW"
      },
      "source": [
        "Model structure note the 32 layers in which our queries would pass through for this particular model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxVMYwWJMJQx",
        "outputId": "152d9037-2c99-4257-bb05-035cc0c193b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jbzQd7Uw_vq",
        "outputId": "8a6595c5-c867-4191-a0df-980f672a7d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response: The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "# Define the input messages\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Answer the following question in a single brief but complete sentence. What is the capital of france\"},\n",
        "]\n",
        "\n",
        "# Apply chat template and tokenize\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "# Define terminators\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id\n",
        "]\n",
        "\n",
        "# Set generation config\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Generate the response with hidden states output\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators[0],  # Ensure eos_token_id is valid\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "    output_hidden_states=True,\n",
        "    return_dict_in_generate=True\n",
        ")\n",
        "\n",
        "# Extract hidden states from the output\n",
        "hidden_states = outputs.hidden_states  # This is a tuple of hidden states\n",
        "\n",
        "response = outputs.sequences[0][input_ids.shape[-1]:]\n",
        "response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
        "print(\"Generated Response:\", response_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rZoxVOWIxMhb"
      },
      "outputs": [],
      "source": [
        "tbg_hiddenstates = hidden_states[0][-5:] # last five layers of first token - should really be last token of X input?\n",
        "slt_hiddenstates = hidden_states[-2][-5:] ## last five layers of second to last token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1t_iarIEiRV"
      },
      "source": [
        "Our 5 hidden states obtained for the second to last token.\n",
        "\n",
        "Lets use the second last tokens across questions to train a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oolbLnjKyRrK",
        "outputId": "39b144db-2dcf-4467-a3e2-2bbd2cdfed78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.0574, 0.1030, 0.2539,  ..., 0.4863, 0.0156, 0.4434]]],\n",
              "        device='cuda:0', dtype=torch.bfloat16),\n",
              " tensor([[[0.0105, 0.1050, 0.3867,  ..., 0.4844, 0.1416, 0.3574]]],\n",
              "        device='cuda:0', dtype=torch.bfloat16),\n",
              " tensor([[[ 0.0312, -0.0723,  0.2275,  ...,  0.6211, -0.0098,  0.2617]]],\n",
              "        device='cuda:0', dtype=torch.bfloat16),\n",
              " tensor([[[-0.0820,  0.0173, -0.0303,  ...,  0.6094,  0.0020, -0.3516]]],\n",
              "        device='cuda:0', dtype=torch.bfloat16),\n",
              " tensor([[[-2.0156,  0.6758, -0.7852,  ...,  4.8750, -0.7227, -2.1875]]],\n",
              "        device='cuda:0', dtype=torch.bfloat16))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "slt_hiddenstates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW0l_oa_59ig",
        "outputId": "9132a926-3c6c-4477-ce2b-2e1cc207f707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4096])\n",
            "torch.Size([4096])\n",
            "torch.Size([4096])\n",
            "torch.Size([4096])\n",
            "torch.Size([4096])\n"
          ]
        }
      ],
      "source": [
        "for hidden_state in slt_hiddenstates:\n",
        "    print(hidden_state.squeeze().shape) # change fom shape [1, 1, 4096] to [4096] as paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59qT6J1YFViG"
      },
      "source": [
        "<h2>Define our Semantic Entropy functions.</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VEtENmsgFPGd"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = AzureOpenAI(\n",
        "            azure_endpoint=os.getenv(\"BASE\"),\n",
        "            api_version=os.getenv(\"VERSION\"),\n",
        "            api_key=os.getenv(\"KEY\"),\n",
        "            timeout=15.0,\n",
        "            max_retries=2\n",
        "        )\n",
        "\n",
        "def make_oai_call(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model= \"gpt-4o\",\n",
        "        temperature=0.0,\n",
        "        max_tokens=2000,\n",
        "        timeout=25.0,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def check_implication_gpt_4o(text1, text2, question):\n",
        "    prompt = f\"\"\"We are evaluating answers to the question \"{question}\"\n",
        "    Here are two possible answers:\n",
        "    Possible Answer 1: {text1}\n",
        "    Possible Answer 2: {text2}\n",
        "    Does Possible Answer 1 semantically entail Possible Answer 2? Respond with entailment, contradiction, or neutral.\"\"\"\n",
        "\n",
        "    response_text = make_oai_call(prompt).lower()\n",
        "    if 'entailment' in response_text:\n",
        "        return 2\n",
        "    elif 'contradiction' in response_text:\n",
        "        return 0\n",
        "    elif 'neutral' in response_text:\n",
        "        return 1\n",
        "    else:\n",
        "        return 1  # Default to neutral if the response is unclear\n",
        "\n",
        "def bidirectional_entailment_clustering(sequences, question):\n",
        "    # Initialize the set of meanings with the first sequence\n",
        "    C = [{sequences[0]}]\n",
        "\n",
        "    # Iterate over each sequence starting from the second one\n",
        "    for m in range(1, len(sequences)):\n",
        "        s_m = sequences[m]\n",
        "        added_to_existing_class = False\n",
        "\n",
        "        # Compare with existing classes\n",
        "        for c in C:\n",
        "            s_c = next(iter(c))  # Get the first sequence in the class\n",
        "\n",
        "            # Check bi-directional entailment\n",
        "            left = check_implication_gpt_4o(s_c, s_m, question) # change call here to other model.\n",
        "            right = check_implication_gpt_4o(s_m, s_c, question)\n",
        "\n",
        "            if left == 2 and right == 2:  # both directions entail\n",
        "                c.add(s_m)\n",
        "                added_to_existing_class = True\n",
        "                break\n",
        "\n",
        "        if not added_to_existing_class:\n",
        "            # If not added to any existing class, create a new class\n",
        "            C.append({s_m})\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "# Function to calculate discrete semantic entropy\n",
        "def calculate_discrete_semantic_entropy(clusters):\n",
        "    total_responses = sum(len(cluster) for cluster in clusters)\n",
        "    probabilities = [len(cluster) / total_responses for cluster in clusters]\n",
        "    print(probabilities)\n",
        "    entropy = -np.sum(probabilities * np.log10(probabilities)) ## log 10\n",
        "    return entropy\n",
        "\n",
        "def calculate_semantic_entropy(responses, question):\n",
        "    clusters = bidirectional_entailment_clustering(responses, question)\n",
        "    for clus in clusters:\n",
        "        print(clus)\n",
        "\n",
        "    # Calculate discrete semantic entropy\n",
        "    entropy = calculate_discrete_semantic_entropy(clusters)\n",
        "    return entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EIDgod4KDZg0"
      },
      "outputs": [],
      "source": [
        "def query_llama_model(prompt):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"Answer the following question in a single brief but complete sentence. {prompt}\"},\n",
        "    ]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    terminators = [tokenizer.eos_token_id]\n",
        "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=terminators[0],  # Ensure eos_token_id is valid\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        output_hidden_states=True,\n",
        "        return_dict_in_generate=True)\n",
        "    hidden_states = outputs.hidden_states  # This is a tuple of hidden states\n",
        "    response = outputs.sequences[0][input_ids.shape[-1]:]\n",
        "    response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
        "    tbg_hiddenstates = hidden_states[0][-5:] #token beginning generation - this should be end of the X input rather then the response generation.\n",
        "    slt_hiddenstates = hidden_states[-2][-5:] # second last token\n",
        "    return response_text, slt_hiddenstates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXoU9vbADZa2",
        "outputId": "8294b7b5-b517-4cdc-edcc-e6e19e011cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Where is the Eiffel Tower?\n",
            "{'The Eiffel Tower is located in Paris, France.'}\n",
            "[1.0]\n",
            "Until when can you continue to use your national ID card to enter the UK in certain cases?\n",
            "{\"You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, when the UK's reciprocal agreement with the EU ends.\", 'You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, as the UK has temporarily allowed the use of national ID cards for travel to the country until that date.', 'You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, as the UK has temporarily extended the acceptance of national ID cards for travel to the UK until that date.'}\n",
            "{'You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, as the UK has not yet decided to accept national ID cards as a valid travel document for entry.'}\n",
            "{\"You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, when the UK's withdrawal from the EU's ID card requirements is expected to take effect.\", \"You can continue to use your national ID card to enter the UK in certain cases until December 31, 2025, as per the UK's withdrawal from the EU's free movement rules.\"}\n",
            "{\"You can continue to use your national ID card to enter the UK in certain cases until October 2021, when the UK's withdrawal from the EU (Brexit) takes effect and the requirement for national ID cards to enter the country ceases to apply.\"}\n",
            "{\"You can continue to use your national ID card to enter the UK in certain cases, such as for travel by air or sea, until January 1, 2021, when the UK's transition period under the Brexit withdrawal agreement ends.\"}\n",
            "{'You can continue to use your national ID card to enter the UK in certain cases until the end of the transition period on December 31, 2020, after which time only a valid passport will be accepted.'}\n",
            "{\"You can continue to use your national ID card to enter the UK in certain cases, such as for travel from Ireland, until January 31, 2021, when the UK's ID card scheme expires.\"}\n",
            "[0.3, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1]\n",
            "What skill level must the job offer be at for skilled workers under the points-based system in the uk?\n",
            "{\"Under the UK's points-based system, a job offer must be at a skill level of RQF (Regulated Qualifications Framework) Level 6 or above, which is equivalent to a bachelor's degree with honors or a master's degree, for skilled workers to be eligible to apply.\", \"Under the UK's points-based system, the job offer must be at a skill level of RQF Level 6 or above (equivalent to a Bachelor's degree or higher) for skilled workers to be eligible.\", \"Under the points-based system in the UK, a job offer must be at a skill level of RQF Level 6 or above (equivalent to a Bachelor's degree or higher) to qualify for skilled workers.\"}\n",
            "{\"Under the points-based system in the UK, a job offer must be at a skill level of RQF Level 6 or above (equivalent to a bachelor's degree or higher) to qualify for skilled worker visas.\", \"Under the UK's points-based system, job offers must be at a skill level of RQF Level 6 (Bachelor's degree or equivalent) or above for skilled workers to be eligible for a visa.\", \"Under the UK's points-based system, a job offer must be at a skill level of RQF Level 6 (Bachelor's degree or equivalent) or above to be eligible for skilled workers.\", \"According to the UK's points-based system, the job offer must be at a skill level of RQF Level 6 or above (equivalent to a bachelor's degree or higher) to qualify for skilled worker visas.\", \"Under the points-based system in the UK, skilled workers must have a job offer with a skill level of RQF Level 6 or above, which is equivalent to a bachelor's degree or higher, to qualify for a visa.\", \"Under the UK's points-based system, skilled workers must have a job offer at a skill level of RQF Level 6 (Bachelor's degree or equivalent) or above to be eligible for a visa.\"}\n",
            "{\"Under the UK's points-based system, a job offer must be at a skill level of RQF Level 6 or above (Master's degree level or equivalent) to qualify for skilled workers.\"}\n",
            "[0.3, 0.6, 0.1]\n",
            "Can EU, EEA, and Swiss citizens use ePassport gates in the UK?\n",
            "{'EU, EEA, and Swiss citizens are not eligible to use ePassport gates in the UK, as they are only available for British and Irish citizens, as well as nationals of the United States, Canada, Australia, New Zealand, and Japan.'}\n",
            "{\"EU, EEA, and Swiss citizens can use ePassport gates in the UK, but only if they are also biometrically enrolled in the UK's biometric database, which is not always the case.\"}\n",
            "{'EU, EEA, and Swiss citizens can use ePassport gates in the UK, but only if they have a biometric chip in their passport and are traveling as a tourist or for business, not for work or study.'}\n",
            "{'No, EU, EEA, and Swiss citizens are not eligible to use ePassport gates in the UK, as they require a biometric residence permit or a valid biometric passport to use these gates, which are not typically held by these groups of citizens.'}\n",
            "{'EU, EEA, and Swiss citizens can use ePassport gates in the UK, but only if they have biometric passports and are exempt from immigration control, such as for short-term business trips or as long-term residents.'}\n",
            "{'EU, EEA, and Swiss citizens can use ePassport gates in the UK, as long as they have biometric passports and are authorized to enter the UK.', 'EU, EEA, and Swiss citizens can use ePassport gates in the UK, but only if they have biometric passports that meet the required standards.'}\n",
            "{'EU, EEA, and Swiss citizens can use ePassport gates in the UK, but only if they are also British citizens or have a valid biometric residence permit.'}\n",
            "{'No, EU, EEA, and Swiss citizens are not eligible to use ePassport gates in the UK, as they are only available to British and Irish citizens, and to non-EU nationals who have biometric passports.'}\n",
            "{'No, EU, EEA, and Swiss citizens are not eligible to use ePassport gates in the UK, as they do not have biometric residence permits and must instead use the standard passport control channels.'}\n",
            "[0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1]\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"Where is the Eiffel Tower?\",\\\n",
        "    \"Until when can you continue to use your national ID card to enter the UK in certain cases?\",\\\n",
        "    \"What skill level must the job offer be at for skilled workers under the points-based system in the uk?\",\\\n",
        "    \"Can EU, EEA, and Swiss citizens use ePassport gates in the UK?\",\n",
        "]\n",
        "\n",
        "features = []\n",
        "targets = []\n",
        "\n",
        "for question in questions:\n",
        "  print(question)\n",
        "  responses = []\n",
        "\n",
        "  for _ in range(10):  # N samples as per the paper\n",
        "      response, slt_hidden_states = query_llama_model(question)\n",
        "      responses.append(response)\n",
        "\n",
        "  entropy = calculate_semantic_entropy(responses, question)\n",
        "\n",
        "  # Collect the hidden states for this question\n",
        "  for hidden_state in slt_hidden_states:\n",
        "      feature_vector = hidden_state.squeeze().to(torch.float32).cpu().numpy()  # Squeeze and convert to numpy array\n",
        "      features.append(feature_vector)\n",
        "      targets.append(entropy)  # Use the same entropy score for all hidden states of this question\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ugYVqCXQo0z",
        "outputId": "2a1b7378-8970-43e6-aec7-20fd1a89d14f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.0,\n",
              " -0.0,\n",
              " -0.0,\n",
              " -0.0,\n",
              " -0.0,\n",
              " 0.796657624451305,\n",
              " 0.796657624451305,\n",
              " 0.796657624451305,\n",
              " 0.796657624451305,\n",
              " 0.796657624451305,\n",
              " 0.38997287335391506,\n",
              " 0.38997287335391506,\n",
              " 0.38997287335391506,\n",
              " 0.38997287335391506,\n",
              " 0.38997287335391506,\n",
              " 0.9397940008672038,\n",
              " 0.9397940008672038,\n",
              " 0.9397940008672038,\n",
              " 0.9397940008672038,\n",
              " 0.9397940008672038]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets # semantic entropy scores for each 5*4 layers first question on paris is 0 entropy so first 5 entries = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_kwDwZsDbtd"
      },
      "source": [
        "<h2>Train a Logistic Regression model on the Hidden States.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scFKi1X_TZd9"
      },
      "source": [
        "Ideally you would collate more of these hiddden states to train more of course! Here its overfitting as only 4 query examples for this quick example. - 3 being 'high' and 1 'low'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqHs06Fd38Rb",
        "outputId": "cdc11bdf-0b39-411c-d75c-e551783394e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (20, 4096)\n",
            "Targets shape: (20,)\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "features = np.array(features)\n",
        "targets = np.array(targets)\n",
        "\n",
        "threshold = 0.2  # Example threshold - the paper has some notation to calculate but seems to be based off intuition still? \\upgamma not defined?\n",
        "\n",
        "categorized_scores = (targets > threshold).astype(int)  # 1 for high, 0 for lo\n",
        "\n",
        "# Check the shapes of features and targets\n",
        "print(f'Features shape: {features.shape}')  # Should be (5, 4096)\n",
        "print(f'Targets shape: {targets.shape}')  # Should be (5,)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, categorized_scores, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model with L2 regularization\n",
        "log_reg = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = log_reg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
