{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "571f3d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045fe3469dc642e8a71021cf3e6bef08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\n",
    "\n",
    "tokenizer.eos_token_id = tokenizer.encode('<|eot_id|>')[0]\n",
    "# response = generate(model, tokenizer, prompt=\"hello\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e9b2534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (k_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (v_proj): QuantizedLinear(input_dims=4096, output_dims=1024, bias=False,group_size=64, bits=4)\n",
       "        (o_proj): QuantizedLinear(input_dims=4096, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "        (down_proj): QuantizedLinear(input_dims=14336, output_dims=4096, bias=False,group_size=64, bits=4)\n",
       "        (up_proj): QuantizedLinear(input_dims=4096, output_dims=14336, bias=False,group_size=64, bits=4)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(4096, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): QuantizedLinear(input_dims=4096, output_dims=128256, bias=False,group_size=64, bits=4)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8ad5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Hello\n",
      " playing the role of the \"bad guy\" in the story. The other characters are all playing the role of the \"good guy\" in the story. The story is about a group of people who are trying to save the world from an alien invasion. The main character, who is playing the role of the \"bad guy,\" is a powerful and evil alien who is leading the invasion. The other characters are all working together to stop the alien and save the world. The story is full of action\n",
      "==========\n",
      "Prompt: 4.912 tokens-per-sec\n",
      "Generation: 13.188 tokens-per-sec\n",
      " playing the role of the \"bad guy\" in the story. The other characters are all playing the role of the \"good guy\" in the story. The story is about a group of people who are trying to save the world from an alien invasion. The main character, who is playing the role of the \"bad guy,\" is a powerful and evil alien who is leading the invasion. The other characters are all working together to stop the alien and save the world. The story is full of action\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello\"\n",
    "\n",
    "response = generate(model, tokenizer, prompt, verbose=True)\n",
    "\n",
    "print(response)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a87d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
